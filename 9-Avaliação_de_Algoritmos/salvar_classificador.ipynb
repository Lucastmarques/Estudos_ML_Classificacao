{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "base = pd.read_csv('credit_data.csv')\n",
    "base.loc[base.age < 0, 'age'] = 40.92\n",
    "               \n",
    "previsores = base.iloc[:, 1:4].values\n",
    "classe = base.iloc[:, 4].values\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(missing_values = np.nan, strategy = 'mean')\n",
    "imputer = imputer.fit(previsores[:, 1:4])\n",
    "previsores[:, 1:4] = imputer.transform(previsores[:, 1:4])\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "previsores = scaler.fit_transform(previsores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\nIteration 395, loss = 0.01370958\nIteration 396, loss = 0.01366098\nIteration 397, loss = 0.01360420\nIteration 398, loss = 0.01354186\nIteration 399, loss = 0.01362411\nIteration 400, loss = 0.01353338\nIteration 401, loss = 0.01339675\nIteration 402, loss = 0.01338103\nIteration 403, loss = 0.01340847\nIteration 404, loss = 0.01338868\nIteration 405, loss = 0.01330481\nIteration 406, loss = 0.01329709\nIteration 407, loss = 0.01320372\nIteration 408, loss = 0.01320145\nIteration 409, loss = 0.01314238\nIteration 410, loss = 0.01318330\nIteration 411, loss = 0.01313908\nIteration 412, loss = 0.01311894\nIteration 413, loss = 0.01334806\nIteration 414, loss = 0.01303155\nIteration 415, loss = 0.01300528\nIteration 416, loss = 0.01290707\nIteration 417, loss = 0.01298062\nIteration 418, loss = 0.01284036\nIteration 419, loss = 0.01279493\nIteration 420, loss = 0.01280756\nIteration 421, loss = 0.01276711\nIteration 422, loss = 0.01272224\nIteration 423, loss = 0.01277034\nIteration 424, loss = 0.01267177\nIteration 425, loss = 0.01265723\nIteration 426, loss = 0.01271084\nIteration 427, loss = 0.01257174\nIteration 428, loss = 0.01251278\nIteration 429, loss = 0.01250901\nIteration 430, loss = 0.01245854\nIteration 431, loss = 0.01249795\nIteration 432, loss = 0.01250983\nIteration 433, loss = 0.01235308\nIteration 434, loss = 0.01245461\nIteration 435, loss = 0.01239142\nIteration 436, loss = 0.01240110\nIteration 437, loss = 0.01226770\nIteration 438, loss = 0.01226713\nIteration 439, loss = 0.01218263\nIteration 440, loss = 0.01221235\nIteration 441, loss = 0.01224342\nIteration 442, loss = 0.01215574\nIteration 443, loss = 0.01209773\nIteration 444, loss = 0.01211007\nIteration 445, loss = 0.01228620\nIteration 446, loss = 0.01206161\nIteration 447, loss = 0.01217606\nIteration 448, loss = 0.01193524\nIteration 449, loss = 0.01193719\nIteration 450, loss = 0.01189910\nIteration 451, loss = 0.01185913\nIteration 452, loss = 0.01185497\nIteration 453, loss = 0.01187510\nIteration 454, loss = 0.01178307\nIteration 455, loss = 0.01178926\nIteration 456, loss = 0.01178575\nIteration 457, loss = 0.01173177\nIteration 458, loss = 0.01174018\nIteration 459, loss = 0.01162175\nIteration 460, loss = 0.01159457\nIteration 461, loss = 0.01157935\nIteration 462, loss = 0.01151672\nIteration 463, loss = 0.01162584\nIteration 464, loss = 0.01147897\nIteration 465, loss = 0.01150086\nIteration 466, loss = 0.01146301\nIteration 467, loss = 0.01141716\nIteration 468, loss = 0.01143788\nIteration 469, loss = 0.01151473\nIteration 470, loss = 0.01129651\nIteration 471, loss = 0.01143139\nIteration 472, loss = 0.01124512\nIteration 473, loss = 0.01129347\nIteration 474, loss = 0.01123583\nIteration 475, loss = 0.01116190\nIteration 476, loss = 0.01114964\nIteration 477, loss = 0.01124579\nIteration 478, loss = 0.01108355\nIteration 479, loss = 0.01108674\nIteration 480, loss = 0.01111168\nIteration 481, loss = 0.01103304\nIteration 482, loss = 0.01099431\nIteration 483, loss = 0.01095680\nIteration 484, loss = 0.01103428\nIteration 485, loss = 0.01106133\nIteration 486, loss = 0.01088468\nIteration 487, loss = 0.01086831\nIteration 488, loss = 0.01088609\nIteration 489, loss = 0.01084140\nIteration 490, loss = 0.01084156\nIteration 491, loss = 0.01082333\nIteration 492, loss = 0.01086469\nIteration 493, loss = 0.01073662\nIteration 494, loss = 0.01073831\nIteration 495, loss = 0.01072292\nIteration 496, loss = 0.01078193\nIteration 497, loss = 0.01063845\nIteration 498, loss = 0.01063213\nIteration 499, loss = 0.01064718\nIteration 500, loss = 0.01051622\nIteration 501, loss = 0.01054875\nIteration 502, loss = 0.01052116\nIteration 503, loss = 0.01044220\nIteration 504, loss = 0.01045537\nIteration 505, loss = 0.01049941\nIteration 506, loss = 0.01045898\nIteration 507, loss = 0.01036822\nIteration 508, loss = 0.01052704\nIteration 509, loss = 0.01035808\nIteration 510, loss = 0.01038232\nIteration 511, loss = 0.01049612\nIteration 512, loss = 0.01024612\nIteration 513, loss = 0.01024525\nIteration 514, loss = 0.01023403\nIteration 515, loss = 0.01022283\nIteration 516, loss = 0.01032520\nIteration 517, loss = 0.01026466\nIteration 518, loss = 0.01019026\nIteration 519, loss = 0.01011459\nIteration 520, loss = 0.01012626\nIteration 521, loss = 0.01010641\nIteration 522, loss = 0.01007868\nIteration 523, loss = 0.01005978\nIteration 524, loss = 0.01007410\nIteration 525, loss = 0.01000966\nIteration 526, loss = 0.01000803\nIteration 527, loss = 0.00994699\nIteration 528, loss = 0.00993925\nIteration 529, loss = 0.00992503\nIteration 530, loss = 0.00987762\nIteration 531, loss = 0.00987721\nIteration 532, loss = 0.00987923\nIteration 533, loss = 0.00985740\nIteration 534, loss = 0.00995351\nIteration 535, loss = 0.00978211\nIteration 536, loss = 0.00996880\nIteration 537, loss = 0.00975896\nIteration 538, loss = 0.00998658\nIteration 539, loss = 0.00967631\nIteration 540, loss = 0.00994659\nIteration 541, loss = 0.00983937\nIteration 542, loss = 0.00968865\nIteration 543, loss = 0.00965249\nIteration 544, loss = 0.00960123\nIteration 545, loss = 0.00967784\nIteration 546, loss = 0.00961464\nIteration 547, loss = 0.00959489\nIteration 548, loss = 0.00952223\nIteration 549, loss = 0.00953190\nIteration 550, loss = 0.00948801\nIteration 551, loss = 0.00948477\nIteration 552, loss = 0.00948643\nIteration 553, loss = 0.00945469\nIteration 554, loss = 0.00939856\nIteration 555, loss = 0.00937347\nIteration 556, loss = 0.00939398\nIteration 557, loss = 0.00936661\nIteration 558, loss = 0.00931014\nIteration 559, loss = 0.00931516\nIteration 560, loss = 0.00930152\nIteration 561, loss = 0.00928742\nIteration 562, loss = 0.00925016\nIteration 563, loss = 0.00917883\nIteration 564, loss = 0.00916500\nIteration 565, loss = 0.00919016\nIteration 566, loss = 0.00916874\nIteration 567, loss = 0.00912382\nIteration 568, loss = 0.00911574\nIteration 569, loss = 0.00911823\nIteration 570, loss = 0.00915697\nIteration 571, loss = 0.00909400\nIteration 572, loss = 0.00906004\nIteration 573, loss = 0.00900705\nIteration 574, loss = 0.00911166\nIteration 575, loss = 0.00911518\nIteration 576, loss = 0.00897794\nIteration 577, loss = 0.00896025\nIteration 578, loss = 0.00899486\nIteration 579, loss = 0.00892636\nIteration 580, loss = 0.00901373\nIteration 581, loss = 0.00894050\nIteration 582, loss = 0.00896215\nIteration 583, loss = 0.00886317\nIteration 584, loss = 0.00886340\nIteration 585, loss = 0.00896710\nIteration 586, loss = 0.00880095\nIteration 587, loss = 0.00884773\nIteration 588, loss = 0.00876764\nIteration 589, loss = 0.00876205\nIteration 590, loss = 0.00872665\nIteration 591, loss = 0.00866791\nIteration 592, loss = 0.00867880\nIteration 593, loss = 0.00865425\nIteration 594, loss = 0.00866300\nIteration 595, loss = 0.00866069\nIteration 596, loss = 0.00873124\nIteration 597, loss = 0.00862811\nIteration 598, loss = 0.00856094\nIteration 599, loss = 0.00864390\nIteration 600, loss = 0.00851972\nIteration 601, loss = 0.00856143\nIteration 602, loss = 0.00853370\nIteration 603, loss = 0.00861318\nIteration 604, loss = 0.00850035\nIteration 605, loss = 0.00845444\nIteration 606, loss = 0.00846335\nIteration 607, loss = 0.00841781\nIteration 608, loss = 0.00841987\nIteration 609, loss = 0.00844535\nIteration 610, loss = 0.00842738\nIteration 611, loss = 0.00837592\nIteration 612, loss = 0.00843557\nIteration 613, loss = 0.00837249\nIteration 614, loss = 0.00834472\nIteration 615, loss = 0.00830162\nIteration 616, loss = 0.00834288\nIteration 617, loss = 0.00825018\nIteration 618, loss = 0.00827596\nIteration 619, loss = 0.00826042\nIteration 620, loss = 0.00828305\nIteration 621, loss = 0.00818198\nIteration 622, loss = 0.00819154\nIteration 623, loss = 0.00826643\nIteration 624, loss = 0.00814131\nIteration 625, loss = 0.00832737\nIteration 626, loss = 0.00815667\nIteration 627, loss = 0.00820089\nIteration 628, loss = 0.00813190\nIteration 629, loss = 0.00812996\nIteration 630, loss = 0.00810096\nIteration 631, loss = 0.00819908\nIteration 632, loss = 0.00798914\nIteration 633, loss = 0.00804784\nIteration 634, loss = 0.00799275\nIteration 635, loss = 0.00808945\nIteration 636, loss = 0.00798279\nIteration 637, loss = 0.00817265\nIteration 638, loss = 0.00796684\nIteration 639, loss = 0.00797669\nIteration 640, loss = 0.00798326\nIteration 641, loss = 0.00796197\nIteration 642, loss = 0.00788475\nIteration 643, loss = 0.00790846\nIteration 644, loss = 0.00798087\nIteration 645, loss = 0.00786080\nIteration 646, loss = 0.00790671\nIteration 647, loss = 0.00787791\nIteration 648, loss = 0.00781199\nIteration 649, loss = 0.00790714\nIteration 650, loss = 0.00783115\nIteration 651, loss = 0.00782731\nIteration 652, loss = 0.00792500\nIteration 653, loss = 0.00779648\nIteration 654, loss = 0.00771715\nIteration 655, loss = 0.00772150\nIteration 656, loss = 0.00768965\nIteration 657, loss = 0.00771464\nIteration 658, loss = 0.00762070\nIteration 659, loss = 0.00773259\nIteration 660, loss = 0.00763017\nIteration 661, loss = 0.00765412\nIteration 662, loss = 0.00761323\nIteration 663, loss = 0.00759905\nIteration 664, loss = 0.00762228\nIteration 665, loss = 0.00761808\nIteration 666, loss = 0.00751256\nIteration 667, loss = 0.00755170\nIteration 668, loss = 0.00755046\nIteration 669, loss = 0.00761155\nIteration 670, loss = 0.00749550\nIteration 671, loss = 0.00766754\nIteration 672, loss = 0.00744063\nIteration 673, loss = 0.00751101\nIteration 674, loss = 0.00744271\nIteration 675, loss = 0.00761191\nIteration 676, loss = 0.00744087\nIteration 677, loss = 0.00754974\nIteration 678, loss = 0.00741242\nIteration 679, loss = 0.00741245\nIteration 680, loss = 0.00736256\nIteration 681, loss = 0.00740399\nIteration 682, loss = 0.00739210\nIteration 683, loss = 0.00734273\nIteration 684, loss = 0.00730818\nIteration 685, loss = 0.00737443\nIteration 686, loss = 0.00725104\nIteration 687, loss = 0.00734668\nIteration 688, loss = 0.00724158\nIteration 689, loss = 0.00727105\nIteration 690, loss = 0.00723167\nIteration 691, loss = 0.00724014\nIteration 692, loss = 0.00722189\nIteration 693, loss = 0.00727982\nIteration 694, loss = 0.00721079\nIteration 695, loss = 0.00719438\nIteration 696, loss = 0.00720823\nIteration 697, loss = 0.00715279\nIteration 698, loss = 0.00715391\nIteration 699, loss = 0.00716043\nIteration 700, loss = 0.00715866\nIteration 701, loss = 0.00727947\nIteration 702, loss = 0.00709867\nIteration 703, loss = 0.00718098\nIteration 704, loss = 0.00704378\nIteration 705, loss = 0.00709656\nIteration 706, loss = 0.00706829\nIteration 707, loss = 0.00707419\nIteration 708, loss = 0.00714850\nIteration 709, loss = 0.00698058\nIteration 710, loss = 0.00703055\nIteration 711, loss = 0.00704090\nIteration 712, loss = 0.00694383\nIteration 713, loss = 0.00699354\nIteration 714, loss = 0.00698715\nIteration 715, loss = 0.00693693\nIteration 716, loss = 0.00706002\nIteration 717, loss = 0.00696916\nIteration 718, loss = 0.00695233\nIteration 719, loss = 0.00691755\nIteration 720, loss = 0.00701276\nIteration 721, loss = 0.00686545\nIteration 722, loss = 0.00692115\nIteration 723, loss = 0.00694555\nIteration 724, loss = 0.00686034\nIteration 725, loss = 0.00688107\nIteration 726, loss = 0.00687040\nIteration 727, loss = 0.00687244\nIteration 728, loss = 0.00676990\nIteration 729, loss = 0.00673715\nIteration 730, loss = 0.00674285\nIteration 731, loss = 0.00675945\nIteration 732, loss = 0.00681080\nIteration 733, loss = 0.00681856\nIteration 734, loss = 0.00673706\nIteration 735, loss = 0.00676094\nIteration 736, loss = 0.00669211\nIteration 737, loss = 0.00675283\nIteration 738, loss = 0.00676451\nIteration 739, loss = 0.00665557\nIteration 740, loss = 0.00667693\nIteration 741, loss = 0.00670736\nIteration 742, loss = 0.00668917\nIteration 743, loss = 0.00659581\nIteration 744, loss = 0.00658646\nIteration 745, loss = 0.00658468\nIteration 746, loss = 0.00657632\nIteration 747, loss = 0.00657789\nIteration 748, loss = 0.00656883\nIteration 749, loss = 0.00658360\nIteration 750, loss = 0.00655033\nIteration 751, loss = 0.00660060\nIteration 752, loss = 0.00654248\nIteration 753, loss = 0.00647766\nIteration 754, loss = 0.00650416\nIteration 755, loss = 0.00649799\nIteration 756, loss = 0.00647891\nIteration 757, loss = 0.00647853\nIteration 758, loss = 0.00648865\nIteration 759, loss = 0.00647231\nIteration 760, loss = 0.00644712\nIteration 761, loss = 0.00644785\nIteration 762, loss = 0.00640047\nIteration 763, loss = 0.00638157\nIteration 764, loss = 0.00641118\nIteration 765, loss = 0.00645570\nIteration 766, loss = 0.00642715\nIteration 767, loss = 0.00640826\nIteration 768, loss = 0.00638720\nIteration 769, loss = 0.00633881\nIteration 770, loss = 0.00631388\nIteration 771, loss = 0.00633261\nIteration 772, loss = 0.00635248\nIteration 773, loss = 0.00629952\nIteration 774, loss = 0.00626916\nIteration 775, loss = 0.00625380\nIteration 776, loss = 0.00634315\nIteration 777, loss = 0.00626503\nIteration 778, loss = 0.00624051\nIteration 779, loss = 0.00630208\nIteration 780, loss = 0.00619974\nIteration 781, loss = 0.00630618\nIteration 782, loss = 0.00642220\nIteration 783, loss = 0.00613636\nIteration 784, loss = 0.00624193\nIteration 785, loss = 0.00618430\nIteration 786, loss = 0.00616413\nIteration 787, loss = 0.00612767\nIteration 788, loss = 0.00614029\nIteration 789, loss = 0.00625287\nIteration 790, loss = 0.00607446\nIteration 791, loss = 0.00614121\nIteration 792, loss = 0.00612986\nIteration 793, loss = 0.00617538\nIteration 794, loss = 0.00604546\nIteration 795, loss = 0.00614037\nIteration 796, loss = 0.00605937\nIteration 797, loss = 0.00619782\nIteration 798, loss = 0.00612622\nIteration 799, loss = 0.00607014\nIteration 800, loss = 0.00603816\nIteration 801, loss = 0.00607134\nIteration 802, loss = 0.00596379\nIteration 803, loss = 0.00620956\nIteration 804, loss = 0.00601298\nIteration 805, loss = 0.00603911\nIteration 806, loss = 0.00609459\nIteration 807, loss = 0.00598707\nIteration 808, loss = 0.00602850\nIteration 809, loss = 0.00599568\nIteration 810, loss = 0.00600763\nIteration 811, loss = 0.00595228\nIteration 812, loss = 0.00597184\nIteration 813, loss = 0.00589131\nIteration 814, loss = 0.00597022\nIteration 815, loss = 0.00587855\nIteration 816, loss = 0.00587662\nIteration 817, loss = 0.00594627\nIteration 818, loss = 0.00585581\nIteration 819, loss = 0.00586068\nIteration 820, loss = 0.00582721\nIteration 821, loss = 0.00593338\nIteration 822, loss = 0.00592341\nIteration 823, loss = 0.00590729\nIteration 824, loss = 0.00580006\nIteration 825, loss = 0.00579637\nIteration 826, loss = 0.00584553\nIteration 827, loss = 0.00573620\nIteration 828, loss = 0.00581105\nIteration 829, loss = 0.00579802\nIteration 830, loss = 0.00590312\nIteration 831, loss = 0.00582697\nIteration 832, loss = 0.00579915\nIteration 833, loss = 0.00575160\nIteration 834, loss = 0.00584808\nIteration 835, loss = 0.00569100\nIteration 836, loss = 0.00577284\nIteration 837, loss = 0.00578966\nIteration 838, loss = 0.00566751\nIteration 839, loss = 0.00572238\nIteration 840, loss = 0.00566866\nIteration 841, loss = 0.00571583\nIteration 842, loss = 0.00575534\nIteration 843, loss = 0.00560992\nIteration 844, loss = 0.00564016\nIteration 845, loss = 0.00560873\nIteration 846, loss = 0.00566631\nIteration 847, loss = 0.00569062\nIteration 848, loss = 0.00558640\nIteration 849, loss = 0.00563146\nIteration 850, loss = 0.00555375\nIteration 851, loss = 0.00559550\nIteration 852, loss = 0.00568203\nIteration 853, loss = 0.00558623\nIteration 854, loss = 0.00562976\nIteration 855, loss = 0.00568377\nIteration 856, loss = 0.00561684\nIteration 857, loss = 0.00552614\nIteration 858, loss = 0.00567576\nIteration 859, loss = 0.00557801\nIteration 860, loss = 0.00558989\nIteration 861, loss = 0.00553376\nIteration 862, loss = 0.00555479\nIteration 863, loss = 0.00547066\nIteration 864, loss = 0.00588635\nIteration 865, loss = 0.00545571\nIteration 866, loss = 0.00548900\nIteration 867, loss = 0.00543791\nIteration 868, loss = 0.00560041\nIteration 869, loss = 0.00565886\nIteration 870, loss = 0.00541668\nIteration 871, loss = 0.00543300\nIteration 872, loss = 0.00540456\nIteration 873, loss = 0.00542134\nIteration 874, loss = 0.00538521\nIteration 875, loss = 0.00540376\nIteration 876, loss = 0.00537841\nIteration 877, loss = 0.00539516\nIteration 878, loss = 0.00535745\nIteration 879, loss = 0.00540715\nIteration 880, loss = 0.00540053\nIteration 881, loss = 0.00545716\nIteration 882, loss = 0.00536467\nIteration 883, loss = 0.00536625\nIteration 884, loss = 0.00543071\nIteration 885, loss = 0.00542841\nIteration 886, loss = 0.00535105\nIteration 887, loss = 0.00527171\nIteration 888, loss = 0.00530423\nIteration 889, loss = 0.00525868\nIteration 890, loss = 0.00531021\nIteration 891, loss = 0.00528787\nIteration 892, loss = 0.00524647\nIteration 893, loss = 0.00530336\nIteration 894, loss = 0.00525366\nIteration 895, loss = 0.00518344\nIteration 896, loss = 0.00524999\nIteration 897, loss = 0.00526456\nIteration 898, loss = 0.00519479\nIteration 899, loss = 0.00520564\nIteration 900, loss = 0.00520162\nIteration 901, loss = 0.00518477\nIteration 902, loss = 0.00520012\nIteration 903, loss = 0.00519833\nIteration 904, loss = 0.00515596\nIteration 905, loss = 0.00523888\nIteration 906, loss = 0.00513941\nIteration 907, loss = 0.00515490\nIteration 908, loss = 0.00516252\nIteration 909, loss = 0.00516106\nIteration 910, loss = 0.00511003\nIteration 911, loss = 0.00510216\nIteration 912, loss = 0.00512866\nIteration 913, loss = 0.00510659\nIteration 914, loss = 0.00524161\nIteration 915, loss = 0.00506645\nIteration 916, loss = 0.00512067\nIteration 917, loss = 0.00509961\nIteration 918, loss = 0.00515754\nIteration 919, loss = 0.00503786\nIteration 920, loss = 0.00509947\nIteration 921, loss = 0.00502634\nIteration 922, loss = 0.00502841\nIteration 923, loss = 0.00504931\nIteration 924, loss = 0.00518041\nIteration 925, loss = 0.00501963\nIteration 926, loss = 0.00504429\nIteration 927, loss = 0.00502514\nIteration 928, loss = 0.00506224\nIteration 929, loss = 0.00501838\nIteration 930, loss = 0.00498358\nIteration 931, loss = 0.00498449\nIteration 932, loss = 0.00494910\nIteration 933, loss = 0.00496424\nIteration 934, loss = 0.00493692\nIteration 935, loss = 0.00495780\nIteration 936, loss = 0.00509168\nIteration 937, loss = 0.00494536\nIteration 938, loss = 0.00497640\nIteration 939, loss = 0.00489238\nIteration 940, loss = 0.00491510\nIteration 941, loss = 0.00494760\nIteration 942, loss = 0.00495413\nIteration 943, loss = 0.00491641\nIteration 944, loss = 0.00491809\nIteration 945, loss = 0.00494460\nIteration 946, loss = 0.00489642\nIteration 947, loss = 0.00488155\nIteration 948, loss = 0.00486732\nIteration 949, loss = 0.00487240\nIteration 950, loss = 0.00508902\nIteration 951, loss = 0.00486430\nIteration 952, loss = 0.00489901\nIteration 953, loss = 0.00483969\nIteration 954, loss = 0.00486768\nIteration 955, loss = 0.00480164\nIteration 956, loss = 0.00485648\nIteration 957, loss = 0.00479061\nIteration 958, loss = 0.00481439\nIteration 959, loss = 0.00482068\nIteration 960, loss = 0.00481475\nIteration 961, loss = 0.00477284\nIteration 962, loss = 0.00483428\nIteration 963, loss = 0.00477385\nIteration 964, loss = 0.00498879\nIteration 965, loss = 0.00484412\nIteration 966, loss = 0.00481765\nIteration 967, loss = 0.00470982\nIteration 968, loss = 0.00475088\nIteration 969, loss = 0.00474678\nIteration 970, loss = 0.00475051\nIteration 971, loss = 0.00476974\nIteration 972, loss = 0.00469230\nIteration 973, loss = 0.00474526\nIteration 974, loss = 0.00482161\nIteration 975, loss = 0.00472907\nIteration 976, loss = 0.00475041\nIteration 977, loss = 0.00467323\nIteration 978, loss = 0.00467185\nIteration 979, loss = 0.00467063\nIteration 980, loss = 0.00467485\nIteration 981, loss = 0.00464447\nIteration 982, loss = 0.00459917\nIteration 983, loss = 0.00471383\nIteration 984, loss = 0.00477152\nIteration 985, loss = 0.00467994\nIteration 986, loss = 0.00466694\nIteration 987, loss = 0.00465536\nIteration 988, loss = 0.00465788\nIteration 989, loss = 0.00470937\nIteration 990, loss = 0.00463132\nIteration 991, loss = 0.00465129\nIteration 992, loss = 0.00458002\nIteration 993, loss = 0.00457783\nIteration 994, loss = 0.00458419\nIteration 995, loss = 0.00457097\nIteration 996, loss = 0.00453248\nIteration 997, loss = 0.00465115\nIteration 998, loss = 0.00466000\nIteration 999, loss = 0.00462633\nIteration 1000, loss = 0.00456410\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "MLPClassifier(activation='relu', alpha=0.0001, batch_size=200, beta_1=0.9,\n              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n              hidden_layer_sizes=100, learning_rate='constant',\n              learning_rate_init=0.001, max_fun=15000, max_iter=1000,\n              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n              power_t=0.5, random_state=None, shuffle=True, solver='adam',\n              tol=1e-05, validation_fraction=0.1, verbose=True,\n              warm_start=False)"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "classificadorSVM = SVC(kernel='rbf', C=2.0)\n",
    "classificadorSVM.fit(previsores, classe)\n",
    "\n",
    "classificadorRF = RandomForestClassifier(n_estimators=40, criterion='entropy')\n",
    "classificadorRF.fit(previsores, classe)\n",
    "\n",
    "classificadorMLP = MLPClassifier(verbose=True, max_iter=1000, tol=0.000010, solver='adam', hidden_layer_sizes=(100),\n",
    "                                 activation='relu', batch_size=200, learning_rate_init=0.001)\n",
    "classificadorMLP.fit(previsores, classe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(classificadorSVM, open('svm_finalizado.sav', 'wb'))\n",
    "pickle.dump(classificadorRF, open('random_forest_finalizado.sav', 'wb'))\n",
    "pickle.dump(classificadorMLP, open('mlp_finalizado.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}