{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redes Neurais Scikit-Learn\n",
    "\n",
    "Um ponto importante para ser discutido nesse notebook, é que o sklearn, por ser uma biblioteca mais abrangente, pode não apredentar resultado esperados para esse tipo de algoritmo. Ainda assim, ela se mostar uma ótima biblioteca, uma vez que automatiza quase todo processo de criação e treinamento da rede neural, resumindo o código a poucas linhas. Além disso, vale a pena fazer os testes com os diferentes tipos de pré-processamento, sempre utilizando o escalonamento e labelencoder para gerar um resultado melhor. Além disso, o scikit learn define automaticamente a função utilizada na saída de acordo com o número de classes diferentes que estamos utilizando. Se forem duas classes => logístico, se forem mais classes => SoftmaxLayer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "base = pd.read_csv('credit_data.csv')\n",
    "base.loc[base.age < 0, 'age'] = base['age'].mean()\n",
    "\n",
    "previsores = base.iloc[:, 1:4]\n",
    "classe = base.iloc[:, 4]\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "previsores = imputer.fit_transform(previsores)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "previsores = scaler.fit_transform(previsores)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "previsores_treinamento, previsores_teste, classe_treinamento, classe_teste = train_test_split(previsores, classe, test_size=0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\nIteration 395, loss = 0.01504698\nIteration 396, loss = 0.01502301\nIteration 397, loss = 0.01496553\nIteration 398, loss = 0.01490202\nIteration 399, loss = 0.01484672\nIteration 400, loss = 0.01481305\nIteration 401, loss = 0.01480917\nIteration 402, loss = 0.01482172\nIteration 403, loss = 0.01468609\nIteration 404, loss = 0.01465650\nIteration 405, loss = 0.01467585\nIteration 406, loss = 0.01458041\nIteration 407, loss = 0.01455815\nIteration 408, loss = 0.01448657\nIteration 409, loss = 0.01446955\nIteration 410, loss = 0.01441587\nIteration 411, loss = 0.01440441\nIteration 412, loss = 0.01435765\nIteration 413, loss = 0.01431083\nIteration 414, loss = 0.01430382\nIteration 415, loss = 0.01433525\nIteration 416, loss = 0.01423823\nIteration 417, loss = 0.01419873\nIteration 418, loss = 0.01413150\nIteration 419, loss = 0.01419150\nIteration 420, loss = 0.01416619\nIteration 421, loss = 0.01402535\nIteration 422, loss = 0.01396721\nIteration 423, loss = 0.01400425\nIteration 424, loss = 0.01392750\nIteration 425, loss = 0.01389176\nIteration 426, loss = 0.01386380\nIteration 427, loss = 0.01392832\nIteration 428, loss = 0.01378992\nIteration 429, loss = 0.01372366\nIteration 430, loss = 0.01373291\nIteration 431, loss = 0.01364480\nIteration 432, loss = 0.01366944\nIteration 433, loss = 0.01360448\nIteration 434, loss = 0.01357839\nIteration 435, loss = 0.01363736\nIteration 436, loss = 0.01351081\nIteration 437, loss = 0.01346107\nIteration 438, loss = 0.01342817\nIteration 439, loss = 0.01344574\nIteration 440, loss = 0.01341517\nIteration 441, loss = 0.01335973\nIteration 442, loss = 0.01337277\nIteration 443, loss = 0.01326613\nIteration 444, loss = 0.01328621\nIteration 445, loss = 0.01318808\nIteration 446, loss = 0.01318204\nIteration 447, loss = 0.01311089\nIteration 448, loss = 0.01310658\nIteration 449, loss = 0.01304207\nIteration 450, loss = 0.01303416\nIteration 451, loss = 0.01301807\nIteration 452, loss = 0.01299420\nIteration 453, loss = 0.01291625\nIteration 454, loss = 0.01291749\nIteration 455, loss = 0.01307102\nIteration 456, loss = 0.01284941\nIteration 457, loss = 0.01288135\nIteration 458, loss = 0.01276562\nIteration 459, loss = 0.01275803\nIteration 460, loss = 0.01271550\nIteration 461, loss = 0.01270283\nIteration 462, loss = 0.01272376\nIteration 463, loss = 0.01268816\nIteration 464, loss = 0.01254884\nIteration 465, loss = 0.01258011\nIteration 466, loss = 0.01256167\nIteration 467, loss = 0.01246913\nIteration 468, loss = 0.01246350\nIteration 469, loss = 0.01242224\nIteration 470, loss = 0.01246578\nIteration 471, loss = 0.01238176\nIteration 472, loss = 0.01243610\nIteration 473, loss = 0.01233169\nIteration 474, loss = 0.01235757\nIteration 475, loss = 0.01229095\nIteration 476, loss = 0.01222530\nIteration 477, loss = 0.01224357\nIteration 478, loss = 0.01220901\nIteration 479, loss = 0.01220985\nIteration 480, loss = 0.01207949\nIteration 481, loss = 0.01207107\nIteration 482, loss = 0.01217079\nIteration 483, loss = 0.01201340\nIteration 484, loss = 0.01198238\nIteration 485, loss = 0.01201439\nIteration 486, loss = 0.01192940\nIteration 487, loss = 0.01190019\nIteration 488, loss = 0.01192787\nIteration 489, loss = 0.01193808\nIteration 490, loss = 0.01192751\nIteration 491, loss = 0.01186298\nIteration 492, loss = 0.01178972\nIteration 493, loss = 0.01174529\nIteration 494, loss = 0.01170952\nIteration 495, loss = 0.01167009\nIteration 496, loss = 0.01177530\nIteration 497, loss = 0.01162667\nIteration 498, loss = 0.01170030\nIteration 499, loss = 0.01165886\nIteration 500, loss = 0.01159790\nIteration 501, loss = 0.01152118\nIteration 502, loss = 0.01161933\nIteration 503, loss = 0.01148426\nIteration 504, loss = 0.01148428\nIteration 505, loss = 0.01147191\nIteration 506, loss = 0.01144867\nIteration 507, loss = 0.01141374\nIteration 508, loss = 0.01137753\nIteration 509, loss = 0.01132384\nIteration 510, loss = 0.01137925\nIteration 511, loss = 0.01136944\nIteration 512, loss = 0.01123100\nIteration 513, loss = 0.01130333\nIteration 514, loss = 0.01143692\nIteration 515, loss = 0.01123173\nIteration 516, loss = 0.01123672\nIteration 517, loss = 0.01111528\nIteration 518, loss = 0.01111330\nIteration 519, loss = 0.01115124\nIteration 520, loss = 0.01109548\nIteration 521, loss = 0.01105647\nIteration 522, loss = 0.01101077\nIteration 523, loss = 0.01099101\nIteration 524, loss = 0.01104468\nIteration 525, loss = 0.01088750\nIteration 526, loss = 0.01090263\nIteration 527, loss = 0.01087890\nIteration 528, loss = 0.01086256\nIteration 529, loss = 0.01086887\nIteration 530, loss = 0.01086625\nIteration 531, loss = 0.01078847\nIteration 532, loss = 0.01076641\nIteration 533, loss = 0.01071823\nIteration 534, loss = 0.01073808\nIteration 535, loss = 0.01076116\nIteration 536, loss = 0.01066987\nIteration 537, loss = 0.01067390\nIteration 538, loss = 0.01070549\nIteration 539, loss = 0.01057741\nIteration 540, loss = 0.01062296\nIteration 541, loss = 0.01056111\nIteration 542, loss = 0.01053058\nIteration 543, loss = 0.01057682\nIteration 544, loss = 0.01048139\nIteration 545, loss = 0.01056724\nIteration 546, loss = 0.01045198\nIteration 547, loss = 0.01049552\nIteration 548, loss = 0.01043707\nIteration 549, loss = 0.01060675\nIteration 550, loss = 0.01034293\nIteration 551, loss = 0.01046110\nIteration 552, loss = 0.01036238\nIteration 553, loss = 0.01036031\nIteration 554, loss = 0.01032760\nIteration 555, loss = 0.01025593\nIteration 556, loss = 0.01021116\nIteration 557, loss = 0.01022102\nIteration 558, loss = 0.01020438\nIteration 559, loss = 0.01017173\nIteration 560, loss = 0.01012882\nIteration 561, loss = 0.01015130\nIteration 562, loss = 0.01010195\nIteration 563, loss = 0.01009243\nIteration 564, loss = 0.01007345\nIteration 565, loss = 0.01018690\nIteration 566, loss = 0.01006131\nIteration 567, loss = 0.01004888\nIteration 568, loss = 0.00999481\nIteration 569, loss = 0.00991729\nIteration 570, loss = 0.00993659\nIteration 571, loss = 0.00988464\nIteration 572, loss = 0.00990350\nIteration 573, loss = 0.00986982\nIteration 574, loss = 0.00986182\nIteration 575, loss = 0.00985344\nIteration 576, loss = 0.00984404\nIteration 577, loss = 0.00979308\nIteration 578, loss = 0.00975907\nIteration 579, loss = 0.00976291\nIteration 580, loss = 0.00972490\nIteration 581, loss = 0.00972004\nIteration 582, loss = 0.00972361\nIteration 583, loss = 0.00971437\nIteration 584, loss = 0.00964127\nIteration 585, loss = 0.00967533\nIteration 586, loss = 0.00966811\nIteration 587, loss = 0.00964509\nIteration 588, loss = 0.00954756\nIteration 589, loss = 0.00956854\nIteration 590, loss = 0.00951316\nIteration 591, loss = 0.00948696\nIteration 592, loss = 0.00945594\nIteration 593, loss = 0.00948357\nIteration 594, loss = 0.00948451\nIteration 595, loss = 0.00942359\nIteration 596, loss = 0.00940529\nIteration 597, loss = 0.00943567\nIteration 598, loss = 0.00934547\nIteration 599, loss = 0.00935328\nIteration 600, loss = 0.00937759\nIteration 601, loss = 0.00937474\nIteration 602, loss = 0.00934210\nIteration 603, loss = 0.00932711\nIteration 604, loss = 0.00928805\nIteration 605, loss = 0.00926501\nIteration 606, loss = 0.00921046\nIteration 607, loss = 0.00919191\nIteration 608, loss = 0.00918075\nIteration 609, loss = 0.00923342\nIteration 610, loss = 0.00920451\nIteration 611, loss = 0.00913328\nIteration 612, loss = 0.00918348\nIteration 613, loss = 0.00917637\nIteration 614, loss = 0.00911945\nIteration 615, loss = 0.00919721\nIteration 616, loss = 0.00908628\nIteration 617, loss = 0.00904723\nIteration 618, loss = 0.00896821\nIteration 619, loss = 0.00900445\nIteration 620, loss = 0.00900283\nIteration 621, loss = 0.00901206\nIteration 622, loss = 0.00899082\nIteration 623, loss = 0.00903413\nIteration 624, loss = 0.00890390\nIteration 625, loss = 0.00894365\nIteration 626, loss = 0.00887636\nIteration 627, loss = 0.00886146\nIteration 628, loss = 0.00879681\nIteration 629, loss = 0.00882011\nIteration 630, loss = 0.00882692\nIteration 631, loss = 0.00885291\nIteration 632, loss = 0.00877561\nIteration 633, loss = 0.00874202\nIteration 634, loss = 0.00871472\nIteration 635, loss = 0.00874131\nIteration 636, loss = 0.00867841\nIteration 637, loss = 0.00865718\nIteration 638, loss = 0.00864509\nIteration 639, loss = 0.00883689\nIteration 640, loss = 0.00869365\nIteration 641, loss = 0.00863347\nIteration 642, loss = 0.00861831\nIteration 643, loss = 0.00855017\nIteration 644, loss = 0.00856781\nIteration 645, loss = 0.00860731\nIteration 646, loss = 0.00857025\nIteration 647, loss = 0.00849418\nIteration 648, loss = 0.00849760\nIteration 649, loss = 0.00851252\nIteration 650, loss = 0.00848417\nIteration 651, loss = 0.00849580\nIteration 652, loss = 0.00846577\nIteration 653, loss = 0.00848503\nIteration 654, loss = 0.00834282\nIteration 655, loss = 0.00838544\nIteration 656, loss = 0.00844815\nIteration 657, loss = 0.00833461\nIteration 658, loss = 0.00835439\nIteration 659, loss = 0.00834593\nIteration 660, loss = 0.00830025\nIteration 661, loss = 0.00831139\nIteration 662, loss = 0.00829054\nIteration 663, loss = 0.00826338\nIteration 664, loss = 0.00822065\nIteration 665, loss = 0.00831886\nIteration 666, loss = 0.00825521\nIteration 667, loss = 0.00830770\nIteration 668, loss = 0.00816530\nIteration 669, loss = 0.00821992\nIteration 670, loss = 0.00828420\nIteration 671, loss = 0.00815613\nIteration 672, loss = 0.00812685\nIteration 673, loss = 0.00808475\nIteration 674, loss = 0.00818052\nIteration 675, loss = 0.00813473\nIteration 676, loss = 0.00809481\nIteration 677, loss = 0.00800435\nIteration 678, loss = 0.00810225\nIteration 679, loss = 0.00804417\nIteration 680, loss = 0.00799195\nIteration 681, loss = 0.00797510\nIteration 682, loss = 0.00796055\nIteration 683, loss = 0.00802335\nIteration 684, loss = 0.00805350\nIteration 685, loss = 0.00796239\nIteration 686, loss = 0.00790654\nIteration 687, loss = 0.00795376\nIteration 688, loss = 0.00785541\nIteration 689, loss = 0.00790701\nIteration 690, loss = 0.00790896\nIteration 691, loss = 0.00789469\nIteration 692, loss = 0.00784913\nIteration 693, loss = 0.00785273\nIteration 694, loss = 0.00780237\nIteration 695, loss = 0.00775113\nIteration 696, loss = 0.00777543\nIteration 697, loss = 0.00773243\nIteration 698, loss = 0.00777893\nIteration 699, loss = 0.00773049\nIteration 700, loss = 0.00781327\nIteration 701, loss = 0.00771794\nIteration 702, loss = 0.00774919\nIteration 703, loss = 0.00772606\nIteration 704, loss = 0.00765004\nIteration 705, loss = 0.00764532\nIteration 706, loss = 0.00768367\nIteration 707, loss = 0.00757300\nIteration 708, loss = 0.00765076\nIteration 709, loss = 0.00758310\nIteration 710, loss = 0.00767487\nIteration 711, loss = 0.00765887\nIteration 712, loss = 0.00753310\nIteration 713, loss = 0.00753372\nIteration 714, loss = 0.00750332\nIteration 715, loss = 0.00749070\nIteration 716, loss = 0.00754760\nIteration 717, loss = 0.00747897\nIteration 718, loss = 0.00750611\nIteration 719, loss = 0.00759367\nIteration 720, loss = 0.00743354\nIteration 721, loss = 0.00739699\nIteration 722, loss = 0.00740290\nIteration 723, loss = 0.00743803\nIteration 724, loss = 0.00741012\nIteration 725, loss = 0.00730991\nIteration 726, loss = 0.00745640\nIteration 727, loss = 0.00734216\nIteration 728, loss = 0.00734912\nIteration 729, loss = 0.00733351\nIteration 730, loss = 0.00723958\nIteration 731, loss = 0.00738637\nIteration 732, loss = 0.00732865\nIteration 733, loss = 0.00731373\nIteration 734, loss = 0.00727447\nIteration 735, loss = 0.00724342\nIteration 736, loss = 0.00720842\nIteration 737, loss = 0.00726183\nIteration 738, loss = 0.00716317\nIteration 739, loss = 0.00723588\nIteration 740, loss = 0.00725439\nIteration 741, loss = 0.00715365\nIteration 742, loss = 0.00712035\nIteration 743, loss = 0.00713931\nIteration 744, loss = 0.00715320\nIteration 745, loss = 0.00724899\nIteration 746, loss = 0.00721533\nIteration 747, loss = 0.00709262\nIteration 748, loss = 0.00709473\nIteration 749, loss = 0.00711068\nIteration 750, loss = 0.00703927\nIteration 751, loss = 0.00706347\nIteration 752, loss = 0.00705418\nIteration 753, loss = 0.00708871\nIteration 754, loss = 0.00708623\nIteration 755, loss = 0.00693791\nIteration 756, loss = 0.00703061\nIteration 757, loss = 0.00696757\nIteration 758, loss = 0.00700558\nIteration 759, loss = 0.00697359\nIteration 760, loss = 0.00692136\nIteration 761, loss = 0.00693511\nIteration 762, loss = 0.00697922\nIteration 763, loss = 0.00693986\nIteration 764, loss = 0.00696991\nIteration 765, loss = 0.00683948\nIteration 766, loss = 0.00691091\nIteration 767, loss = 0.00689239\nIteration 768, loss = 0.00685953\nIteration 769, loss = 0.00693314\nIteration 770, loss = 0.00680800\nIteration 771, loss = 0.00684131\nIteration 772, loss = 0.00680446\nIteration 773, loss = 0.00676963\nIteration 774, loss = 0.00677250\nIteration 775, loss = 0.00676386\nIteration 776, loss = 0.00678140\nIteration 777, loss = 0.00670515\nIteration 778, loss = 0.00674239\nIteration 779, loss = 0.00665933\nIteration 780, loss = 0.00674331\nIteration 781, loss = 0.00669081\nIteration 782, loss = 0.00667491\nIteration 783, loss = 0.00673733\nIteration 784, loss = 0.00672330\nIteration 785, loss = 0.00671063\nIteration 786, loss = 0.00661941\nIteration 787, loss = 0.00665118\nIteration 788, loss = 0.00663623\nIteration 789, loss = 0.00662171\nIteration 790, loss = 0.00658743\nIteration 791, loss = 0.00655477\nIteration 792, loss = 0.00654029\nIteration 793, loss = 0.00653546\nIteration 794, loss = 0.00653028\nIteration 795, loss = 0.00652182\nIteration 796, loss = 0.00650981\nIteration 797, loss = 0.00650980\nIteration 798, loss = 0.00648777\nIteration 799, loss = 0.00654697\nIteration 800, loss = 0.00646216\nIteration 801, loss = 0.00645996\nIteration 802, loss = 0.00646625\nIteration 803, loss = 0.00643946\nIteration 804, loss = 0.00645209\nIteration 805, loss = 0.00649874\nIteration 806, loss = 0.00639456\nIteration 807, loss = 0.00639203\nIteration 808, loss = 0.00637033\nIteration 809, loss = 0.00637657\nIteration 810, loss = 0.00640180\nIteration 811, loss = 0.00641377\nIteration 812, loss = 0.00634579\nIteration 813, loss = 0.00624669\nIteration 814, loss = 0.00651730\nIteration 815, loss = 0.00637151\nIteration 816, loss = 0.00632038\nIteration 817, loss = 0.00633334\nIteration 818, loss = 0.00629351\nIteration 819, loss = 0.00633048\nIteration 820, loss = 0.00635799\nIteration 821, loss = 0.00627497\nIteration 822, loss = 0.00626183\nIteration 823, loss = 0.00623475\nIteration 824, loss = 0.00620583\nIteration 825, loss = 0.00625295\nIteration 826, loss = 0.00624824\nIteration 827, loss = 0.00621943\nIteration 828, loss = 0.00618069\nIteration 829, loss = 0.00623097\nIteration 830, loss = 0.00628103\nIteration 831, loss = 0.00621608\nIteration 832, loss = 0.00614228\nIteration 833, loss = 0.00616726\nIteration 834, loss = 0.00613302\nIteration 835, loss = 0.00619319\nIteration 836, loss = 0.00612004\nIteration 837, loss = 0.00608811\nIteration 838, loss = 0.00609564\nIteration 839, loss = 0.00605064\nIteration 840, loss = 0.00607477\nIteration 841, loss = 0.00613494\nIteration 842, loss = 0.00606493\nIteration 843, loss = 0.00608813\nIteration 844, loss = 0.00605046\nIteration 845, loss = 0.00599506\nIteration 846, loss = 0.00600500\nIteration 847, loss = 0.00602533\nIteration 848, loss = 0.00601057\nIteration 849, loss = 0.00600205\nIteration 850, loss = 0.00594971\nIteration 851, loss = 0.00593753\nIteration 852, loss = 0.00596219\nIteration 853, loss = 0.00588700\nIteration 854, loss = 0.00594821\nIteration 855, loss = 0.00589455\nIteration 856, loss = 0.00594937\nIteration 857, loss = 0.00588376\nIteration 858, loss = 0.00588764\nIteration 859, loss = 0.00583220\nIteration 860, loss = 0.00586658\nIteration 861, loss = 0.00587177\nIteration 862, loss = 0.00597738\nIteration 863, loss = 0.00584248\nIteration 864, loss = 0.00590694\nIteration 865, loss = 0.00587908\nIteration 866, loss = 0.00579369\nIteration 867, loss = 0.00579053\nIteration 868, loss = 0.00580169\nIteration 869, loss = 0.00585835\nIteration 870, loss = 0.00581351\nIteration 871, loss = 0.00582820\nIteration 872, loss = 0.00588063\nIteration 873, loss = 0.00576430\nIteration 874, loss = 0.00580920\nIteration 875, loss = 0.00579131\nIteration 876, loss = 0.00569774\nIteration 877, loss = 0.00569196\nIteration 878, loss = 0.00572119\nIteration 879, loss = 0.00575897\nIteration 880, loss = 0.00570521\nIteration 881, loss = 0.00568602\nIteration 882, loss = 0.00566406\nIteration 883, loss = 0.00567542\nIteration 884, loss = 0.00570495\nIteration 885, loss = 0.00565407\nIteration 886, loss = 0.00560759\nIteration 887, loss = 0.00568695\nIteration 888, loss = 0.00559285\nIteration 889, loss = 0.00563778\nIteration 890, loss = 0.00566648\nIteration 891, loss = 0.00556969\nIteration 892, loss = 0.00558734\nIteration 893, loss = 0.00559290\nIteration 894, loss = 0.00553467\nIteration 895, loss = 0.00560248\nIteration 896, loss = 0.00554823\nIteration 897, loss = 0.00553003\nIteration 898, loss = 0.00554617\nIteration 899, loss = 0.00555508\nIteration 900, loss = 0.00551916\nIteration 901, loss = 0.00551150\nIteration 902, loss = 0.00553447\nIteration 903, loss = 0.00554059\nIteration 904, loss = 0.00554138\nIteration 905, loss = 0.00544243\nIteration 906, loss = 0.00552376\nIteration 907, loss = 0.00546784\nIteration 908, loss = 0.00543670\nIteration 909, loss = 0.00544900\nIteration 910, loss = 0.00543266\nIteration 911, loss = 0.00543680\nIteration 912, loss = 0.00542737\nIteration 913, loss = 0.00538289\nIteration 914, loss = 0.00538210\nIteration 915, loss = 0.00540173\nIteration 916, loss = 0.00542485\nIteration 917, loss = 0.00539495\nIteration 918, loss = 0.00542928\nIteration 919, loss = 0.00536994\nIteration 920, loss = 0.00531649\nIteration 921, loss = 0.00531253\nIteration 922, loss = 0.00530452\nIteration 923, loss = 0.00531128\nIteration 924, loss = 0.00536534\nIteration 925, loss = 0.00526212\nIteration 926, loss = 0.00531276\nIteration 927, loss = 0.00525208\nIteration 928, loss = 0.00531662\nIteration 929, loss = 0.00532673\nIteration 930, loss = 0.00529143\nIteration 931, loss = 0.00522597\nIteration 932, loss = 0.00526889\nIteration 933, loss = 0.00523622\nIteration 934, loss = 0.00522543\nIteration 935, loss = 0.00521139\nIteration 936, loss = 0.00542772\nIteration 937, loss = 0.00525039\nIteration 938, loss = 0.00529719\nIteration 939, loss = 0.00535899\nIteration 940, loss = 0.00530129\nIteration 941, loss = 0.00522775\nIteration 942, loss = 0.00515331\nIteration 943, loss = 0.00511525\nIteration 944, loss = 0.00513829\nIteration 945, loss = 0.00523996\nIteration 946, loss = 0.00519430\nIteration 947, loss = 0.00525523\nIteration 948, loss = 0.00527669\nIteration 949, loss = 0.00514301\nIteration 950, loss = 0.00506747\nIteration 951, loss = 0.00518483\nIteration 952, loss = 0.00504743\nIteration 953, loss = 0.00512491\nIteration 954, loss = 0.00509610\nIteration 955, loss = 0.00502116\nIteration 956, loss = 0.00514040\nIteration 957, loss = 0.00507896\nIteration 958, loss = 0.00499059\nIteration 959, loss = 0.00503441\nIteration 960, loss = 0.00513782\nIteration 961, loss = 0.00502933\nIteration 962, loss = 0.00506618\nIteration 963, loss = 0.00501175\nIteration 964, loss = 0.00508439\nIteration 965, loss = 0.00497807\nIteration 966, loss = 0.00496651\nIteration 967, loss = 0.00503269\nIteration 968, loss = 0.00498116\nIteration 969, loss = 0.00495471\nIteration 970, loss = 0.00502103\nIteration 971, loss = 0.00510364\nIteration 972, loss = 0.00499252\nIteration 973, loss = 0.00495736\nIteration 974, loss = 0.00491074\nIteration 975, loss = 0.00497991\nIteration 976, loss = 0.00489035\nIteration 977, loss = 0.00492131\nIteration 978, loss = 0.00494758\nIteration 979, loss = 0.00494925\nIteration 980, loss = 0.00494208\nIteration 981, loss = 0.00488239\nIteration 982, loss = 0.00486566\nIteration 983, loss = 0.00485945\nIteration 984, loss = 0.00483446\nIteration 985, loss = 0.00502773\nIteration 986, loss = 0.00492677\nIteration 987, loss = 0.00482878\nIteration 988, loss = 0.00487093\nIteration 989, loss = 0.00480585\nIteration 990, loss = 0.00481220\nIteration 991, loss = 0.00478344\nIteration 992, loss = 0.00493108\nIteration 993, loss = 0.00488629\nIteration 994, loss = 0.00480650\nIteration 995, loss = 0.00475073\nIteration 996, loss = 0.00483870\nIteration 997, loss = 0.00476252\nIteration 998, loss = 0.00486545\nIteration 999, loss = 0.00470150\nIteration 1000, loss = 0.00487414\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n              hidden_layer_sizes=100, learning_rate='constant',\n              learning_rate_init=0.001, max_fun=15000, max_iter=1000,\n              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n              power_t=0.5, random_state=None, shuffle=True, solver='adam',\n              tol=1e-05, validation_fraction=0.1, verbose=True,\n              warm_start=False)"
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "classificador = MLPClassifier(verbose=True, #Mostra erro por iteração\n",
    "                              max_iter=1000, #Define o iterador máximo\n",
    "                              tol=0.00001, #Define a tolerância (se após dez épocas o erro diminuir menos que a tolerancia, iteração para)\n",
    "                              solver='adam', #\n",
    "                              hidden_layer_sizes=(100), #Número de hidden layers com o número de neurônios. Ex.: (100, 100) 2 camadas com 100 neurônios\n",
    "                              activation='relu') #Função utilizada (retified linear)\n",
    "classificador.fit(previsores_treinamento, classe_treinamento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "previsoes = classificador.predict(previsores_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Precisão:  0.996\n[[435   1]\n [  1  63]]\n"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "precisao = accuracy_score(classe_teste, previsoes)\n",
    "matriz = confusion_matrix(classe_teste, previsoes)\n",
    "print('Precisão: ', precisao)\n",
    "print(matriz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}