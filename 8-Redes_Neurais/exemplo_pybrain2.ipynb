{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criando Rede Neural com PyBrain\n",
    "\n",
    "As redes neurais ganharam muita força no mercado com a utilização do DeepLearning. Por mais que esse algoritmo tenha um custo operacional muito alto, ele é o melhor para a resolução de problemas complexos. Resumidamente o objetivo desse algoritmo é achar a combinação de pesos que entregue o menor valor do erro. Para isso, o algoritmo irá usar uma série de cálculos envolvendo os erros, o gradiente da função (para achar o mínimo global na função erro) e um delta.\n",
    "\n",
    "De forma simplificada o que o algoritmo faz submeter vários valores a várias funções em forma de camadas, divididas entre camada de entrada (camada que contém os valores dos previsores), camada oculta (valores após uma série de cálculos) e camada de saída (valores da previsão). Cada camada é formada por neurônios que guardam consigo um determiando valor numérico, por isso, ao trabalharmos com esse algoritmo, é importante tranformarmos os valores das variáveis categóricas em valores numéricos. O algortimo funciona da seguinte forma: ele realiza o somatório do produto de cada neurônio da camada de entrada pelo seu respectivo peso, joga esse valor numa função (normalmente a Sigmoide, mas podemos utilizar outras como a função degrau e tangente hiperbólica) gerando assim um novo valor que, por sua vez, estará contido em cada neurônio da camada oculta, esse processo pode se repetir $n$ vezes, sendo $n$ o número de camadas ocultas, até que esses valores sejam submetidos a um novo somatório de produtos, aplicado a uma nova função (normalmente Sigmoide também) gerando assim o(s) valor(es) do(s) neurônio(s) de saída.\n",
    "\n",
    "Normalmente, nos algoritmos de classificação, os neurônios da camada de saída funcionam de forma binária, isto é, cada neurônio deve informar valores muito próximos de 0 ou 1. isso acontece justamente pelo fato de um neurônio só guardar valores numéricos. Assim, para resolver os problemas com mais de 2 tipos de classes, podemos codificar nossa classe, como por exemplo, risco alto, médio e baixo, como sendo um número binário, ou melhor, um conjunto de 0 e 1, tal que, podemos dizer que risco alto = 1 0 0, risco médio = 0 1 0 e risco baixo = 0 0 1, analogamente ao que fizemos no OneHotEncode, ao tranformar um atributo em vários outros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pybrain.tools.shortcuts import buildNetwork\n",
    "from pybrain.datasets import SupervisedDataSet\n",
    "from pybrain.supervised.trainers import BackpropTrainer\n",
    "from pybrain.structure.modules import SoftmaxLayer\n",
    "from pybrain.structure.modules import SigmoidLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<LinearLayer 'in'>\n<SigmoidLayer 'hidden0'>\n<SoftmaxLayer 'out'>\nNone\n"
    }
   ],
   "source": [
    "# E X E M P L O\n",
    "### Criando rede de forma simplificada. Cada parâmetro corresponde a quantidade de neurônios de cada camada respectivamente (entrada, oculta(s), saida)\n",
    "rede = buildNetwork(2, 3, 1,\n",
    "                    outclass=SoftmaxLayer, #Altera função da camada de saída\n",
    "                    hiddenclass=SigmoidLayer, #Alterafunções das camadas ocultas\n",
    "                    bias=False) #Retira o Bias\n",
    "### Imprime a função da camada de entrada\n",
    "print(rede['in'])\n",
    "### Imprime a função da camada oculta\n",
    "print(rede['hidden0'])\n",
    "### Imprime a função da camada de saída\n",
    "print(rede['out'])\n",
    "### Imprime a unidade de bias\n",
    "print(rede['bias'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[0. 0.]\n [0. 1.]\n [1. 0.]\n [1. 1.]]\n[[0.]\n [1.]\n [1.]\n [0.]]\n"
    }
   ],
   "source": [
    "### Criando rede de forma simplificada\n",
    "rede = buildNetwork(2, 3, 1)\n",
    "### Cria base de dados (exemplo de uma porta XOR) com 2 atributos previsores e 1 classe\n",
    "base = SupervisedDataSet(2, 1)\n",
    "### Adicionando elementos à base\n",
    "base.addSample((0, 0), (0, ))\n",
    "base.addSample((0, 1), (1, ))\n",
    "base.addSample((1, 0), (1, ))\n",
    "base.addSample((1, 1), (0, ))\n",
    "### Imprime os dados de entrada e as classes meta\n",
    "print(base['input'])\n",
    "print(base['target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quanto menor o learningrate, menor será o erro. No entanto, isso irá ter um custo computacional, ou seja, pode ser que demore muito para o algoritmo ser compilado e executado. A mesma coisa acontece com o momentum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Erro: 0.127618666505632\nErro: 0.1268157426741901\nErro: 0.12591208849045604\nErro: 0.12463439581906756\nErro: 0.1220451323982357\nErro: 0.11725007225830561\nErro: 0.10958095503990226\nErro: 0.09858886658560345\nErro: 0.08359992823707686\nErro: 0.06096260227984304\nErro: 0.029773771630862336\nErro: 0.008140396066159749\nErro: 0.0013475718885801538\nErro: 0.00017176118720165393\nErro: 1.971646636142006e-05\nErro: 2.1854782049930134e-06\nErro: 2.388915809607794e-07\nErro: 2.5988703489920717e-08\nErro: 2.830526290176739e-09\nErro: 3.0724320863700425e-10\nErro: 3.345624341838106e-11\nErro: 3.6364599870218747e-12\nErro: 3.9459546218059864e-13\nErro: 4.293854645027884e-14\nErro: 4.6672128701537796e-15\nErro: 5.072844586190637e-16\nErro: 5.514060700043973e-17\nErro: 5.990047623021708e-18\nErro: 6.519585159597404e-19\n"
    }
   ],
   "source": [
    "### Fazendo o treinamento\n",
    "treinamento = BackpropTrainer(rede, dataset=base, learningrate=0.01, momentum=0.06)\n",
    "### Controlando o número de épocas do treinamento\n",
    "for i in range(1, 30000):\n",
    "    erro = treinamento.train()\n",
    "    # Imprime o erro a cada 1000 gerações\n",
    "    if i%1000 == 0:\n",
    "        print(\"Erro: {}\".format(erro))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[1.99498112e-10]\n[1.]\n[1.]\n[5.2219809e-10]\n"
    }
   ],
   "source": [
    "### Imprimindo algumas previsões\n",
    "print(rede.activate([0, 0]))\n",
    "print(rede.activate([1, 0]))\n",
    "print(rede.activate([0, 1]))\n",
    "print(rede.activate([1, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}