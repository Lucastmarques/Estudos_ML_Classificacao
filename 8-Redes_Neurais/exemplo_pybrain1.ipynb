{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criando Rede Neural com PyBrain\n",
    "\n",
    "As redes neurais ganharam muita força no mercado com a utilização do DeepLearning. Por mais que esse algoritmo tenha um custo operacional muito alto, ele é o melhor para a resolução de problemas complexos. Resumidamente o objetivo desse algoritmo é achar a combinação de pesos que entregue o menor valor do erro. Para isso, o algoritmo irá usar uma série de cálculos envolvendo os erros, o gradiente da função (para achar o mínimo global na função erro) e um delta.\n",
    "\n",
    "De forma simplificada o que o algoritmo faz submeter vários valores a várias funções em forma de camadas, divididas entre camada de entrada (camada que contém os valores dos previsores), camada oculta (valores após uma série de cálculos) e camada de saída (valores da previsão). Cada camada é formada por neurônios que guardam consigo um determiando valor numérico, por isso, ao trabalharmos com esse algoritmo, é importante tranformarmos os valores das variáveis categóricas em valores numéricos. O algortimo funciona da seguinte forma: ele realiza o somatório do produto de cada neurônio da camada de entrada pelo seu respectivo peso, joga esse valor numa função (normalmente a Sigmoide, mas podemos utilizar outras como a função degrau e tangente hiperbólica) gerando assim um novo valor que, por sua vez, estará contido em cada neurônio da camada oculta, esse processo pode se repetir $n$ vezes, sendo $n$ o número de camadas ocultas, até que esses valores sejam submetidos a um novo somatório de produtos, aplicado a uma nova função (normalmente Sigmoide também) gerando assim o(s) valor(es) do(s) neurônio(s) de saída.\n",
    "\n",
    "Normalmente, nos algoritmos de classificação, os neurônios da camada de saída funcionam de forma binária, isto é, cada neurônio deve informar valores muito próximos de 0 ou 1. isso acontece justamente pelo fato de um neurônio só guardar valores numéricos. Assim, para resolver os problemas com mais de 2 tipos de classes, podemos codificar nossa classe, como por exemplo, risco alto, médio e baixo, como sendo um número binário, ou melhor, um conjunto de 0 e 1, tal que, podemos dizer que risco alto = 1 0 0, risco médio = 0 1 0 e risco baixo = 0 0 1, analogamente ao que fizemos no OneHotEncode, ao tranformar um atributo em vários outros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Importando o modelo que será utilizado na nossa rede neural (feed forward)\n",
    "from pybrain.structure import FeedForwardNetwork\n",
    "### Importando os tipos de funções que iremos utilizar nas nossas layers, bem como o \"atributo\" bias (adiciona um nó a mais para cada layer da nossa rede)\n",
    "from pybrain.structure import LinearLayer, SigmoidLayer, BiasUnit\n",
    "### Importando a função que irá de fato fazer a conexão dos nossos neurônios\n",
    "from pybrain.structure import FullConnection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Cria rede neural\n",
    "rede = FeedForwardNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Cira camada de entrada com 2 neurônios\n",
    "# Linear layer significa que não iremos submeter a nenhum tipo de função (os valores vão sem modificações)\n",
    "camadaEntrada = LinearLayer(2)\n",
    "### Cria camada oculta com função sigmoide de 3 neurônios\n",
    "camadaOculta = SigmoidLayer(3)\n",
    "### Cria camada de saída com função sigmoide com 1 neurônio\n",
    "camadaSaida = SigmoidLayer(1)\n",
    "### Cria bias para as camadas de entrada e de saída\n",
    "bias1 = BiasUnit()\n",
    "bias2 = BiasUnit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Adicionando as camadas à nossa rede neural\n",
    "rede.addModule(camadaEntrada)\n",
    "rede.addModule(camadaOculta)\n",
    "rede.addModule(camadaSaida)\n",
    "rede.addModule(bias1)\n",
    "rede.addModule(bias2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Fazendo a ligação entre a camada de entrada e a camada oculta\n",
    "entradaOculta = FullConnection(camadaEntrada, camadaOculta)\n",
    "### Fazendo a ligação entre a camada oculta e a camada de saída\n",
    "ocultaSaida = FullConnection(camadaOculta, camadaSaida)\n",
    "### Ligando os bias às camadas de entrada e saída\n",
    "biasOculta = FullConnection(bias1, camadaOculta)\n",
    "biasSaida = FullConnection(bias2, camadaSaida)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Efetiva a construção da rede neural\n",
    "rede.sortModules()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "FeedForwardNetwork-5\n   Modules:\n    [<BiasUnit 'BiasUnit-6'>, <BiasUnit 'BiasUnit-7'>, <LinearLayer 'LinearLayer-9'>, <SigmoidLayer 'SigmoidLayer-10'>, <SigmoidLayer 'SigmoidLayer-8'>]\n   Connections:\n    []\n\n[ 0.61087518  2.11290946 -1.13064875 -0.63100327 -0.1323307  -0.61857915]\n[ 0.07252894  1.52138576 -1.04088944]\n[0.21371814 0.52873366 0.33796274]\n[1.28995991]\n"
    }
   ],
   "source": [
    "print(rede)\n",
    "print(entradaOculta.params)\n",
    "print(ocultaSaida.params)\n",
    "print(biasOculta.params)\n",
    "print(biasSaida.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}